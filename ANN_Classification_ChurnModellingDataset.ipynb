{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac668d61",
   "metadata": {},
   "source": [
    "#### Dataset information:\n",
    "\n",
    "Here we are dealing with a dataset from the finance domain. We have a dataset where we are having 14 dimensions in total and 100000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60edc61e",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "\n",
    "Here our main goal is to create an artificial neural network that will take into consideration all independent variables(first 13) and based on that will predict if our customer is going to exit the bank or not(Exited is dependent variable here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fca24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d52fd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "\n",
    "data = pd.read_csv(\"D:/FSDS-iNeuron/3.Resource/Dataset/Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb0eb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6371c4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56cafd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2edac95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Generating Matrix of Features\n",
    "\n",
    "X = data.iloc[:,3:-1].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56663d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([665, 'France', 'Female', 40, 6, 0.0, 1, 1, 1, 161848.03],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01aaebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating Dependent Variable Vectors\n",
    "\n",
    "Y = data.iloc[:,-1].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b04f6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical Variable Gender\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "LE1 = LabelEncoder()\n",
    "X[:,2] = np.array(LE1.fit_transform(X[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c894b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([619, 'France', 0, 42, 2, 0.0, 1, 1, 1, 101348.88], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4014591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical variable Geography\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "ct =ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4528fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 0.0, 0.0, 619, 0, 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7685a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be613fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d415aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising ANN\n",
    "\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07db60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding First Hidden Layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
    "\n",
    "#Adding Second Hidden Layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
    "\n",
    "#Adding Output Layer\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d85f249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling ANN\n",
    "\n",
    "ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cc7be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000024163235700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000024163235700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "250/250 [==============================] - 5s 3ms/step - loss: 0.5837 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4980 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4855 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4696 - accuracy: 0.7960\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4544 - accuracy: 0.7960\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4448 - accuracy: 0.7960\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4384 - accuracy: 0.7960\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4343 - accuracy: 0.7960\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4311 - accuracy: 0.7960\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4291 - accuracy: 0.7960\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4275 - accuracy: 0.7960\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.7960\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4240 - accuracy: 0.7960\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4219 - accuracy: 0.7960\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4197 - accuracy: 0.7996\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4163 - accuracy: 0.8011\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4131 - accuracy: 0.8041\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4082 - accuracy: 0.8080\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4026 - accuracy: 0.8159\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3957 - accuracy: 0.8276\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3888 - accuracy: 0.8345\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3829 - accuracy: 0.8393\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3776 - accuracy: 0.8450\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3733 - accuracy: 0.8496\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3698 - accuracy: 0.8491\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3662 - accuracy: 0.8534\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8537\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3609 - accuracy: 0.8550\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3590 - accuracy: 0.8564\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8556\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3547 - accuracy: 0.8575\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3537 - accuracy: 0.8593\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3522 - accuracy: 0.8587\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3510 - accuracy: 0.8584\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3502 - accuracy: 0.8620\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3492 - accuracy: 0.8614\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3483 - accuracy: 0.8605\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8639\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3465 - accuracy: 0.8615\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3458 - accuracy: 0.8620\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3451 - accuracy: 0.8620\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3443 - accuracy: 0.8629\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3437 - accuracy: 0.8621\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3436 - accuracy: 0.8631\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3424 - accuracy: 0.8620\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3420 - accuracy: 0.8630\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3417 - accuracy: 0.8630\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8625\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3400 - accuracy: 0.8629\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3405 - accuracy: 0.8627\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3401 - accuracy: 0.8645\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8618\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3391 - accuracy: 0.8619\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8622\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8634\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3390 - accuracy: 0.8633\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3385 - accuracy: 0.8635\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3387 - accuracy: 0.8636\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3388 - accuracy: 0.8626\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3378 - accuracy: 0.8620\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8624\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3378 - accuracy: 0.8619\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3372 - accuracy: 0.8626\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3375 - accuracy: 0.8626\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3374 - accuracy: 0.8634\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8637\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3375 - accuracy: 0.8614\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3368 - accuracy: 0.8636\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3368 - accuracy: 0.8637\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3371 - accuracy: 0.8622\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3365 - accuracy: 0.8625\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3363 - accuracy: 0.8615\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8637\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3363 - accuracy: 0.8625\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.8630\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3358 - accuracy: 0.8636\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3363 - accuracy: 0.8625\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3358 - accuracy: 0.8637\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8650\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.8630\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8626\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3357 - accuracy: 0.8641\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8634\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8631\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3355 - accuracy: 0.8629\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3355 - accuracy: 0.8633\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8650\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3349 - accuracy: 0.8633\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.8626\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3353 - accuracy: 0.8615\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3347 - accuracy: 0.8620\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3347 - accuracy: 0.8624\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.8612\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8649\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3356 - accuracy: 0.8625\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3345 - accuracy: 0.8626\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3344 - accuracy: 0.8622\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3344 - accuracy: 0.8625\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8627\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241632f4e50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting ANN\n",
    "\n",
    "ann.fit(X_train,Y_train,batch_size=32,epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4181e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sai\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002416346B160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002416346B160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#Predicting result for Single Observation\n",
    "\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1,50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4543e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving created neural network\n",
    "\n",
    "ann.save(\"ANN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac6726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
